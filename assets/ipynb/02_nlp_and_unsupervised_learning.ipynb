{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering UK research projects to thematic topics using text data\n",
    "\n",
    "This tutorial looks at the use of natural language processing and unsupervised learning to cluster projects in the Gateway to Research database to thematic topics.  \n",
    "\n",
    "We will learn how to preprocess text data, transform words to numbers, use dimensionality reduction to project the documents on a 2D space and cluster them.\n",
    "\n",
    "## What we will do:\n",
    "* Import the Python modules that will be used in the analysis. \n",
    "* Read the Gateway to Research data.\n",
    "* Do some very basic data cleaning and exploratory data analysis.\n",
    "* Preprocess the abstracts of the research projects.\n",
    "* Train word embeddings using word2vec and run some example queries.\n",
    "* Create document vectors by averaging word vectors.\n",
    "* Use t-SNE to reduce the dimensionality of the document vectors to 2D.\n",
    "* Cluster the document vectors using Gaussian Mixtures.\n",
    "\n",
    "## How is this tutorial structured:\n",
    "For every section, I will highlight its **Goal** and **what we will do** to achieve it. Then, I will **explain the methods** we use, **what alternatives or additional thing we could do** and lastly, we will **run the code** together. Note that some code cells can \"run\" for a while, so we will run them first and then explain what they do.\n",
    "\n",
    "**Feel free to ask questions at any point of the talk!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade git+https://github.com/nestauk/im_tutorials.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "### These are the Python libraries we will use throughout the tutorial. Let's describe the main ones:\n",
    "\n",
    "**Numpy**: Widely used package for scientific computing with Python. If something is an array/matrix, we do it with Numpy!  \n",
    "**Pandas**: Like Excel, but for Python.  \n",
    "**Matplotlib**: The go-to data visualisation library in Python.  \n",
    "**Scikit-learn**: One of the main libraries for machine learning in Python.  \n",
    "**NLTK**: A well established Python package to work with human language data.\n",
    "\n",
    "### We also created a few modules for this tutorial:  \n",
    "**text_preprocessing**: Contains the functions to preprocess the research abstracts.  \n",
    "**dim_reduction**: Wrappers and helper functions for t-SNE and Gaussian Mixtures.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "%matplotlib inline\n",
    "\n",
    "import ast\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from im_tutorials.data import *\n",
    "from im_tutorials.utilities import flatten_lists\n",
    "from im_tutorials.features.text_preprocessing import *\n",
    "from im_tutorials.features.document_vectors import document_vector\n",
    "from im_tutorials.features.dim_reduction import WrapTSNE, GaussianMixtureEval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data\n",
    "\n",
    "The data for this project is stored as a CSV file on Amazon Web Services (AWS) S3, a static cloud file storage service. We can use pandas to pull the data directly into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data \n",
    "gtr_projects_df = gtr.gateway_to_research_projects()\n",
    "\n",
    "# Drop duplicates\n",
    "gtr_projects_df.drop_duplicates('project_id', inplace=True)\n",
    "\n",
    "# Shape of our data\n",
    "print('The Gateway to Research data has {} rows and {} columns'.format(gtr_projects_df.shape[0], gtr_projects_df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic exploratory data analysis\n",
    "\n",
    "### Goal\n",
    "Explore the dataset, do some very basic cleaning and prepare it for further analysis.\n",
    "\n",
    "### We will:\n",
    "* Find the missing values and remove them\n",
    "* See how our data look like\n",
    "* Plot how many project were submitted every year.\n",
    "* Find which Research Councils have funded the most projects.\n",
    "* Examine the length of the abstract of the research projects (we are creating new features here!).\n",
    "\n",
    "All of the above will be done with Pandas and Matplotlib!\n",
    "\n",
    "### Other things we could do:\n",
    "* Spot outliers\n",
    "* Normalise some data fields\n",
    "* Basic statistics for the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values\n",
    "gtr_projects_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing data\n",
    "gtr_projects_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_projects_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(nrows=2, figsize=(15,8))\n",
    "\n",
    "# ax1\n",
    "# Count the projects per year and sort them from older to newer ones. \n",
    "gtr_projects_df.start_year.value_counts().reindex(sorted(gtr_projects_df.start_year.value_counts().index)).plot(ax=ax1)\n",
    "ax1.set_title('Number of research projects', fontsize=15)\n",
    "\n",
    "# ax2 \n",
    "# Group by the data on 'start_year' and 'funder_name' and count the number of projects belonging to each of these groups.\n",
    "gtr_projects_df.groupby(['start_year', 'funder_name']).count()['project_id'].unstack('funder_name').plot(kind='bar', stacked=True, ax=ax2)\n",
    "ax2.set_title('Funded projects by Research Council', fontsize=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the average length of the abstracts\n",
    "gtr_projects_df['abstract_texts_length'] = gtr_projects_df.abstract_texts.apply(lambda doc: len(doc))\n",
    "\n",
    "# Plot\n",
    "f, (ax1, ax2) = plt.subplots(nrows=2, figsize=(15,8))\n",
    "\n",
    "# ax1\n",
    "# Group by the projects on the 'start_year' and average the length of their research abstracts.\n",
    "gtr_projects_df.groupby('start_year').mean()['abstract_texts_length'].plot(ax=ax1)\n",
    "ax1.set_title('Average length of research abstracts', fontsize=15)\n",
    "\n",
    "# ax2 \n",
    "# Group by the projects on the 'start_year' and 'funder_name' and average the length of the research abstracts for each group.\n",
    "gtr_projects_df.groupby(['start_year', 'funder_name']).mean()['abstract_texts_length'].unstack('funder_name').plot(kind='bar', stacked=True, ax=ax2)\n",
    "ax2.set_title('Average length of research abstracts by Research Council', fontsize=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing\n",
    "\n",
    "### Goal\n",
    "\n",
    "We will clean the abstract of every research project. \n",
    "\n",
    "### We will:\n",
    "* Lowercase and tokenise the documents.\n",
    "* Keep only tokens that have more than two characters.\n",
    "* Filter out stop words, punctuation, numeric and non-English characters.\n",
    "* Create n-grams.\n",
    "\n",
    "### Some definitions\n",
    "\n",
    "**Natural language processing**: It's a subfield of computer science and information engineering (and very influenced by linguistics) concerned with how to program computers to process and analyse natural language data.\n",
    "**Tokenisation:** It is a particular kind of document segmentation. Segmentation breaks up text into smaller chunks or segments, with more focused information content. Tokens are instances of a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing.  \n",
    "**n-grams**: Sequence of concatenated tokens.\n",
    "**Vocabulary:** A collection of the unique tokens found in a collection of documents.  \n",
    "**One-hot-vectors:** A representation of tokens as binary vectors. The length of the vector is equal to the size of the vocabulary.  \n",
    "\n",
    "<img src=\"../reports/figures/onehot.png\" width=\"600\">\n",
    "\n",
    "### Other things that you could try:\n",
    "* Change the regular expression used to tokenise the documents.\n",
    "* Stem or lemmatise tokens based on their Part-of-Speech tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Process the abstracts\n",
    "gtr_projects_df['processed_documents'] = build_ngrams([flatten_lists(tokenize_document(document)) for document in list(gtr_projects_df.abstract_texts)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary column showing if a document has any tokens left after preprocessing\n",
    "gtr_projects_df['is_Doc'] = gtr_projects_df.processed_documents.apply(lambda doc: 1 if len(doc) != 0 else 0)\n",
    "\n",
    "# Keep only rows with a processed abstract\n",
    "gtr_projects_df = gtr_projects_df[gtr_projects_df.is_Doc == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create word vectors\n",
    "\n",
    "### Goal\n",
    "Represent the words with vectors.\n",
    "\n",
    "### We will:\n",
    "\n",
    "Use [word2vec](https://radimrehurek.com/gensim/models/word2vec.html) to create a dense vector representation of tokens and project them to a low-dimensional vector space. This is done in a way that similar tokens will be closer on the vector space.\n",
    "\n",
    "### Why not using something simpler?\n",
    "Few disadvantages of one-hot-vectors:\n",
    "* Very high dimensionality -> takes longer to train ML models.\n",
    "* They do not learn based on their context, meaning that we cannot find relations between tokens.\n",
    "\n",
    "<img src=\"../reports/figures/onehotencoding.png\">\n",
    "\n",
    "### What's word2vec?\n",
    "\n",
    "Word2Vec is a shallow neural network which takes as input a large collection of sentences and produces a vector space of words, typically of several hundred dimensions where each word is assigned to corresponding vector in the space. Word vectors are positioned in the vector space in a way that those sharing similar context are located in close proximity to one another in the high dimensional vector space.  \n",
    "\n",
    "<img src=\"../reports/figures/word2vec.png\" width=\"800\">\n",
    "\n",
    "Word2Vec is a particularly computationally-efficient predictive model for learning word embeddings from raw text.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "<img src=\"../reports/figures/w2v_architectures.png\" width=\"800\">\n",
    "\n",
    "### Word2vec: Few important hyperparameters\n",
    "* **`size`**: Dimensionality of the word vectors (length of the hidden layer).\n",
    "* **`min_count`**: Ignores all words with total frequency lower than this.\n",
    "* **`window`**: Maximum distance between the current and predicted word within a sentence.\n",
    "* **`learning rate`**: The learning rate used in training.\n",
    "* We can choose between the **`skip-gram`** and **`Continuous BOW`** implementation of the algorithm.\n",
    "* We can choose **`Hierachical Softmax`** or **`Negative Sampling`** to train of the algorithm.\n",
    "\n",
    "\n",
    "### Other methods you could use:\n",
    "* One-hot-encodings\n",
    "* Use a pre-trained word2vec model\n",
    "* [TF-IDF](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "* [GloVe](https://nlp.stanford.edu/projects/glove/)\n",
    "* [fastText](https://fasttext.cc/)\n",
    "* [ELMo](https://allennlp.org/elmo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "w2v = gensim.models.Word2Vec(list(gtr_projects_df.processed_documents), size=300, window=10, min_count=2, iter=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some examples of word2vec\n",
    "\n",
    "We can query the trained word2vec with a token from its vocabulary and find the ones that are most similar to it. The float number of every \"row\" shows the cosine similarity between the query and the token on the left of the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.most_similar(['virtual_reality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.most_similar(['blockchain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.most_similar(['artificial_intelligence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.most_similar(['machine_learning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.most_similar(['big_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.most_similar(['social_sciences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.most_similar(['particle_physics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create document vectors\n",
    "\n",
    "### Goal \n",
    "Create document vectors!\n",
    "\n",
    "### We will:\n",
    "Average the word vectors of a document to create document vectors. The words that are not in word2vec model's vocabulary are ignored.\n",
    "\n",
    "### Other methods you could use:\n",
    "* Averaged word vectors weighted by their TF-IDF.\n",
    "* [doc2vec](https://radimrehurek.com/gensim/models/doc2vec.html)\n",
    "* [sent2vec](https://rare-technologies.com/sent2vec-an-unsupervised-approach-towards-learning-sentence-embeddings/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a sample of the data to make the second part of the tutorial run faster\n",
    "gtr_projects_df = gtr_projects_df.sample(8000)\n",
    "\n",
    "# Average word vectors\n",
    "doc_vecs = [document_vector(doc=processed_document, word2vec_model=w2v) for processed_document in list(gtr_projects_df.processed_documents)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster vectorised documents \n",
    "\n",
    "### Goal\n",
    "Reduce the dimensionality of the document vectors and cluster them.\n",
    "\n",
    "### We will:\n",
    "* Find the cosine distance of the document vector matrix.\n",
    "* Reduce its dimensionality to 2D using [t-SNE](https://lvdmaaten.github.io/tsne/).\n",
    "* Cluster the points of the 2D space using [Gaussian Mixtures](https://scikit-learn.org/stable/modules/mixture.html)\n",
    "\n",
    "### t-SNE\n",
    "t-SNE is a nonlinear dimensionality reduction technique well-suited for embedding high-dimensional data in a low-dimensional space (usually 2D).  \n",
    "\n",
    "In the high-dimensional space, t-SNE creates a probability distribution that dictates the relationships between various neighbouring points. Then, it tries to recreate a low dimensional space that follows that probability distribution as best as possible.\n",
    "\n",
    "<img src=\"../reports/figures/tsne.png\" width=\"800\">\n",
    "\n",
    "#### t-SNE: Few important hyperparameters\n",
    "* **`n_components`**: Dimension of the embedded space.\n",
    "* **`perplexity`:** The perplexity is related to the number of nearest neighbors that is used in other manifold learning algorithms. Larger datasets usually require a larger perplexity. Consider selecting a value between 5 and 50. The choice is not extremely critical since t-SNE is quite insensitive to this parameter. The performance of t-SNE is fairly robust under different settings of the perplexity. \n",
    "* **`learning_rate`:** If the learning rate is too high, the data may look like a ‘ball’ with any point approximately equidistant from its nearest neighbours. If the learning rate is too low, most points may look compressed in a dense cloud with few outliers. If the cost function gets stuck in a bad local minimum increasing the learning rate may help.\n",
    "\n",
    "### Gaussian Mixtures\n",
    "A Gaussian mixture model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters.\n",
    "\n",
    "<img alt=\"Credit: KazukiAmakawa [CC BY-SA 4.0 (https://creativecommons.org/licenses/by-sa/4.0)]\" src=\"../reports/figures/gaussian_mixture.gif\" width=\"800\">\n",
    "\n",
    "#### GMM: Few important hyperparameters\n",
    "* **`n_components`**: The number of mixture components.\n",
    "* **`covariance_type`**: The type of covariance to use.\n",
    "\n",
    "<img src=\"../reports/figures/covariance_types.png\" width=\"800\">\n",
    "\n",
    "### Other methods we could use:\n",
    "* PCA (for dimensionality reduction)\n",
    "* k-Means (for clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Dimensionality reduction with t-SNE\n",
    "ts = WrapTSNE()\n",
    "tsne_space = ts.reduce_dimensions(doc_vecs, n_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering with GMM\n",
    "# Instantiate the class using the 2D space from t-SNE\n",
    "np.random.seed(42)\n",
    "gmm_eval = GaussianMixtureEval(tsne_space)\n",
    "\n",
    "best_gmm, bic = gmm_eval.fit_eval(max_components=40)\n",
    "print('BEST GMM MODEL: {}'.format(best_gmm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict token clusters based on best_gmm\n",
    "token_labels = best_gmm.predict(tsne_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.scatter(tsne_space[:,0], tsne_space[:,1], c=token_labels)\n",
    "plt.title('Document vector space', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "In this tutorial, we've seen how to preprocess text data, find the vector representation of documents using word embeddings, reduced their dimensionality to 2D with t-SNE and cluster them with Gaussian Mixtures.\n",
    "\n",
    "### Where can we go from here?\n",
    "* Tryout the methods we mentioned at each part of the tutorial. Most of them will definitely improve the results!\n",
    "* The methods we showcased can -in principle- be used with any text data. How would you use them in your work?\n",
    "\n",
    "### Few things to consider\n",
    "* **Scalability of the methods.** We decided to use less documents in t-SNE and GMMs in order to reduce the computing time.\n",
    "* It's **difficult to evaluate** the results of unsupervised learning methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
